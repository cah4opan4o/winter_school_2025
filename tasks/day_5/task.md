**Задание 1. Спланировать “Учебный MapReduce” для одного примера**
1. Определите, какие данные будем обрабатывать. Возможные варианты:
    * **WordCount**: возьмите какой-то текст (симитируйте “посты пользователей”), разбейте на 3–5 кусков.
    * **Граф**: можно “map” сделать (вершина, 1), reduce = суммирование (получим общее число вершин?).
    * **Логи**: можно сгенерировать “(userID, actionType, timestamp)”, map: “(actionType, 1)”, reduce: суммарная статистика.
2. Пусть Master хранит N=3 “части” файла (или массива).
3. Заведите 3 узла “map worker”, 2 узла “reduce worker”, + 1 “master” (всего 6 горутин, например).
4. **Opционально**: использовать уже знакомый **лидер** / Raft, чтобы master был “надежный”.

**Задание 2. Реализовать MapTask, ReduceTask (очень упрощённо)**
1. В файле mapreduce.go, заведите структуры:
```go
type MapTask struct {
    chunkID   int
    data      string // or []byte
}

type ReduceTask struct {
    key   string
    values []int // or []string, depends on aggregator
}
```

2. **MapFunc** (например func mapFunc(data string) []Pair)
    * Разбиваем строку на слова, выдаём (слово, 1).
3. **ReduceFunc** (например func reduceFunc(key string, values []int) (string, int))
    * Суммируем values → (key, total).

**Задание 3. “Master” горутина: распределение, shuffle, reduce**
1. В main_day5.go (или другом файле) сделайте “Master” в виде горутины, которая:
    * Имеет map\[int\]string “chunks” (N кусков).
    * Рассылает MapTask узлам-MapWorker.
    * Собирать результат (list of (k, v) пар) от каждого MapWorker.
2. **Shuffle**:
    * Группируем все (k, v) по ключу k.
    * Распределяем их по ReduceWorker’ам (например, hash(k) mod R = workerIndex).
3. “ReduceWorker” принимает (k, []v), вызывает reduceFunc. Возвращает (k, result).
4. Master собирает финальные (k, result) и печатает.

**Задание 4. Перезапуск Map-задачи при сбое worker**
1. В текущем коде, если Master поручил “MapTask(chunkID)” worker-у “W1”, и W1 “падает” (alive=false), Master не получает ответа.
2. **Введём timeout**: если ответа по каналу “mapResultChan” нет за X секунд, Master решает, что W1 упал.
3. **Перезапускаем** MapTask на другом worker (W2).
4. **W2** (реалистично) берёт chunk, выполняет mapFunc, присылает (k, v) пары.
5. Убедитесь, что Master не дублирует, то есть если 2 worker всё же прислали результаты, Master может “слиять” или “убедиться”, что “W1” уже не жив.
  
**Реализация**:
* В  mapreduce.go внесите поле taskStatus\[chunkID\] = "in-progress"/"done".
* При назначении задачи MapWorker-у ставим “in-progress”.
* Если нет ответа → назначаем другому. Когда получаем ответ, отмечаем “done”.

**Задание 5. Реализовать Combiner**
1. В mapFunc, кроме выдачи (k,1) для каждого слова (WordCount), можете локально “комбинировать” эти пары:
    * Сохранять во map[string]int, и в конце mapFunc выдавать уже агрегированные (k, sum).
2. Увидите, что в Shuffle меньше данных.

**Задание 6. “Лидер Master”
1. Используйте уже знакомый алгоритм **лидер-элекции** (Bully или Ring-based).
2. Пусть N узлов, все потенциальные “MasterCandidates”. После выборов у нас есть 1 Leader, 4 Follower, скажем.
3. **Лидер** выполняет роль Master MapReduce:
    * Рассылает MapTasks, собирает результаты.
    * Рассылает ReduceTasks, собирает результаты.
4. Если лидер “падает” (alive=false), другой узел становится лидером. Но, чтобы сохранить прогресс MapReduce, нужно хранить “metadata” (какие chunk’и уже готовы, какие reduce’ы сделаны). Можно хранить это в mini-Raft-лог, но это усложнение (для “full consistency”).

**Задание 7. Собрать «проект» целиком и навести порядок в коде**
1. **Разделить** файлы/пакеты логично:
    * graph/: graph.go, bfs_dfs.go, mst.go, shortestpath.go, unionfind.go
    * dist/: node.go, leader_bully.go, leader_ring.go, raft.go
    * mapreduce/: mapreduce.go (master, worker, combiner), examples.go
    * cmd/: где main_* файлы (демо каждого дня).
2. Убедиться, что каждый модуль импортирует уже написанные решения (не дублируем код).
3. **Документировать** ключевые структуры и функции (GoDoc формат — // BFS выполняет обход...).

**Задание 8. Подготовить демонстрационный пример «end-to-end»**
1. Создайте **“demo_main.go”**, который покажет:

   **8.1. Локальную аналитику:**
    * Инициализировать Graph, добавить вершины/рёбра,
    * Запустить DFS/BFS, вывести порядок обхода,
    * Запустить KruskalMST, вывести MST.
    * Запустить Dijkstra, вывести dist.
      **8.2. Запуск «кластерных» узлов (goroutines):**
    * 5 узлов, “лидер-элекция” (Bully или Ring).
    * Подождать, пока выберется лидер, вывести: “Лидер = …”.
   
      **8.3. “mini-Raft”:**
    * Используете Raft-версию, где «лидер» — Raft leader, и фолловеры.
    * Послать 1–2 «команды» (допустим “addFriend(A,B)”).
    * Наблюдать AppendEntries, лог у всех.
   
      **8.4. MapReduce:**
    * Запустить master (либо “лидер” в роли master).
    * Разбить какой-то “текст” (или “лог”) на 3 части, послать map-works, shuffle, reduce.
    * Вывести финальный результат (например, wordcount).
   
      **8.5. Тест на сбои (минимальный)**
    * Во время MapReduce, “убить” одного map worker (alive = false). Наблюдать, что “master” перезапускает задачу.
    * При “mini-Raft”, убить “лидера” после 1-й команды. Наблюдать переизбрание, потом 2-ю команду послать новому лидеру. Сравнить логи.
    * Вывести «final logs» всех узлов.